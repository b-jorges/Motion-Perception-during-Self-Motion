g <- read.table(header=T,"PilotData/Discarded/Pilots07.txt")
h <- read.table(header=T,"PilotData/pilots04.txt")
i <- read.table(header=T,"PilotData/Pilots06.txt")
j <- read.table(header=T,"PilotData/Pilots05.txt")
b$id = "s02_2D"
c$id = "s01_2D"
d$id = "s01_3D"
e$id = "s03"
b$Start_Above = 1
c$Start_Above = 1
f$id = "s02_3D"
g$id = "s07"
h$id = "s04"
i$id = "s05"
j$id = "s06"
a = rbind(b,c,d,e,f,g,h,i,j)
a = a %>%
mutate(
Pest_Bigger = case_when(
Response_Interval == Pest_Interval ~ 1,
Response_Interval != Pest_Interval ~ 0,
),
Direction = case_when(
velH < 0 ~ "left",
velH > 0 ~ "right",
),
Difference = abs(velH_Pest)-abs(velH),
velH_Absolut = abs(velH),
Congruent = case_when(
velH*velH_Subject < 0 ~ "incongruent",
velH*velH_Subject > 0 ~ "congruent",
velH*velH_Subject == 0 ~ "1no motion"
),
Difference_Percent = Difference/velH
) %>%
filter(abs(velH_Pest) < abs(velH)*1.5)
a = a %>%
group_by(id,Congruent,velH,Difference) %>%
mutate(Yes = sum(Pest_Bigger==1),
Total = length(velH_Subject))
Data_GLM =
select(a,c(id,Congruent,velH,Difference,Yes,Total,velH_Subject)) %>%
distinct()
Data_GLM = Data_GLM %>%
mutate(
Static = case_when(
velH_Subject == 0 ~ 1,
velH_Subject != 0 ~ 0
)
)
#####Plot data raw data for all subjects (tested in 3D)
ggplot(a[a$id %in% c("s01_3D", "s02_3D", "s03", "s04", "s05", "s06", "s07"),],
aes ( x = Difference, y = Pest_Bigger, col = as.factor(Congruent))) +
binomial_smooth() +
facet_grid(id~velH)
ggsave("PlotsPilotData.jpg", w=10, h=10)
mod1 = glmer(cbind(Yes, Total - Yes) ~ Congruent*Difference + (Difference | id) + (Difference | velH),
family = binomial(link = "probit"),
data = Data_GLM[Data_GLM$id %in% c("s01_3D", "s02_3D", "s03", "s04", "s05", "s06"),])
mod2 = glmer(cbind(Yes, Total - Yes) ~ Congruent + Difference + (Difference | id)  + (Difference | velH),
family = binomial(link = "probit"),
data = Data_GLM[Data_GLM$id %in% c("s01_3D", "s02_3D", "s03", "s04", "s05", "s06"),])
summary(mod3)
anovNa(mod4,mod3)
mod1 = glmer(cbind(Yes, Total - Yes) ~ Congruent*Difference + (Difference | id) + (Difference | velH),
family = binomial(link = "probit"),
data = Data_GLM[Data_GLM$id %in% c("s01_3D", "s02_3D", "s03", "s04", "s05", "s06"),])
mod2 = glmer(cbind(Yes, Total - Yes) ~ Congruent + Difference + (Difference | id)  + (Difference | velH),
family = binomial(link = "probit"),
data = Data_GLM[Data_GLM$id %in% c("s01_3D", "s02_3D", "s03", "s04", "s05", "s06"),])
summary(mod1)
anova(mod1,mod2)
mod3 = glmer(cbind(Yes, Total - Yes) ~ Congruent + (Difference | id) + (Difference | velH),
family = binomial(link = "probit"),
data = Data_GLM[Data_GLM$id %in% c("s01_3D", "s02_3D", "s03", "s04", "s05", "s06"),])
mod4 = glmer(cbind(Yes, Total - Yes) ~ (Difference | id)  + (Difference | velH),
family = binomial(link = "probit"),
data = Data_GLM[Data_GLM$id %in% c("s01_3D", "s02_3D", "s03", "s04", "s05", "s06"),])
summary(mod3)
anova(mod3,mod4)
summary(mod3)
summary(mod1)
a = a %>%
mutate(
Pest_Bigger = case_when(
Response_Interval == Pest_Interval ~ 1,
Response_Interval != Pest_Interval ~ 0,
),
Direction = case_when(
velH < 0 ~ "left",
velH > 0 ~ "right",
),
Difference = abs(velH_Pest)-abs(velH),
velH_Absolut = abs(velH),
Congruent = case_when(
velH*velH_Subject < 0 ~ "incongruent",
velH*velH_Subject > 0 ~ "congruent",
velH*velH_Subject == 0 ~ "1no motion"
),
Difference_Percent = Difference/velH,
SelfMotionPresent = case_when(
velH_Subject == 0 ~ "no",
velH_Subject != 0 ~ "yes"
)
) %>%
filter(abs(velH_Pest) < abs(velH)*1.5)
a = rbind(b,c,d,e,f,g,h,i,j)
a = a %>%
mutate(
Pest_Bigger = case_when(
Response_Interval == Pest_Interval ~ 1,
Response_Interval != Pest_Interval ~ 0,
),
Direction = case_when(
velH < 0 ~ "left",
velH > 0 ~ "right",
),
Difference = abs(velH_Pest)-abs(velH),
velH_Absolut = abs(velH),
Congruent = case_when(
velH*velH_Subject < 0 ~ "incongruent",
velH*velH_Subject > 0 ~ "congruent",
velH*velH_Subject == 0 ~ "1no motion"
),
Difference_Percent = Difference/velH,
SelfMotionPresent = case_when(
velH_Subject == 0 ~ "no",
velH_Subject != 0 ~ "yes"
)
) %>%
filter(abs(velH_Pest) < abs(velH)*1.5)
a = a %>%
group_by(id,Congruent,velH,Difference) %>%
mutate(Yes = sum(Pest_Bigger==1),
Total = length(velH_Subject))
Data_GLM =
select(a,c(id,Congruent,velH,Difference,Yes,Total,velH_Subject)) %>%
distinct()
Data_GLM = Data_GLM %>%
mutate(
Static = case_when(
velH_Subject == 0 ~ 1,
velH_Subject != 0 ~ 0
)
)
#####Plot data raw data for all subjects (tested in 3D)
ggplot(a[a$id %in% c("s01_3D", "s02_3D", "s03", "s04", "s05", "s06", "s07"),],
aes ( x = Difference, y = Pest_Bigger, col = as.factor(Congruent))) +
binomial_smooth() +
facet_grid(id~velH)
ggsave("PlotsPilotData.jpg", w=10, h=10)
mod1 = glmer(cbind(Yes, Total - Yes) ~ SelfMotionPresent*Difference + (Difference | id) + (Difference | velH),
family = binomial(link = "probit"),
data = Data_GLM[Data_GLM$id %in% c("s01_3D", "s02_3D", "s03", "s04", "s05", "s06"),])
mod2 = glmer(cbind(Yes, Total - Yes) ~ SelfMotionPresent + Difference + (Difference | id)  + (Difference | velH),
family = binomial(link = "probit"),
data = Data_GLM[Data_GLM$id %in% c("s01_3D", "s02_3D", "s03", "s04", "s05", "s06"),])
summary(mod1)
anova(mod1,mod2)
Data_GLM =
select(a,c(id,Congruent,velH,Difference,Yes,Total,velH_Subject,SelfMotionPresent)) %>%
distinct()
Data_GLM = Data_GLM %>%
mutate(
Static = case_when(
velH_Subject == 0 ~ 1,
velH_Subject != 0 ~ 0
)
)
#####Plot data raw data for all subjects (tested in 3D)
ggplot(a[a$id %in% c("s01_3D", "s02_3D", "s03", "s04", "s05", "s06", "s07"),],
aes ( x = Difference, y = Pest_Bigger, col = as.factor(Congruent))) +
binomial_smooth() +
facet_grid(id~velH)
ggsave("PlotsPilotData.jpg", w=10, h=10)
mod1 = glmer(cbind(Yes, Total - Yes) ~ SelfMotionPresent*Difference + (Difference | id) + (Difference | velH),
family = binomial(link = "probit"),
data = Data_GLM[Data_GLM$id %in% c("s01_3D", "s02_3D", "s03", "s04", "s05", "s06"),])
mod2 = glmer(cbind(Yes, Total - Yes) ~ SelfMotionPresent + Difference + (Difference | id)  + (Difference | velH),
family = binomial(link = "probit"),
data = Data_GLM[Data_GLM$id %in% c("s01_3D", "s02_3D", "s03", "s04", "s05", "s06"),])
summary(mod1)
anova(mod1,mod2)
summary(mod1)
###Pull the whole repository and paste the path to the the local GitHub repository here:
require(dplyr)
require(tidyverse)
require(lme4)
library(PearsonDS)
theme_set(theme_cowplot())
set.seed(912)
#####This is the function from which I choose which comparison values we put into the modelling part:
#super high curtosis because we will get many values around the PSE
DataFrame3 = data.frame(x = rcauchy(1000,1,0.02),
y = rnorm(1000,1,0.1))
ggplot(DataFrame3, aes(x)) + ####just to get an idea of what this function looks like
geom_density() +
coord_cartesian(xlim=c(0.7,1.3))
ID = paste0("s",1:16)
Motion = c(-2,0,2) ##motion left, right, and static ... this will be factors that are supposed to represent congruent/incongruent
velH = c(-8,-6.6, 6.6,8) ##target motion to the left (neg values) and to the right (pos values)
reps = seq(1,55,1) ##we have two PESTS, and between 20 and 35 reps for each, averages for 55 for both together
SimulatePsychometricFunction = function(ID,Motion,velH, reps,PSE_Diff, JND_Diff){
Psychometric = expand.grid(ID=ID, Motion=Motion, velH=velH, reps = reps)
Psychometric = Psychometric %>%
group_by(ID) %>%#
mutate(PSE_Factor_ID = rnorm(1,1,0.15),
SD_Factor_ID = rnorm(1,1,0.15))
Psychometric = Psychometric %>%
group_by(ID,Motion,velH) %>%
mutate(
EffectOfSelfMotion.Accuracy = (2*velH/3+Motion*PSE_Diff)*abs(PSE_Factor_ID), ###get PSE: +/- 1/8 of selfmotion, aaaand some inter-subject variability (PSE_Factor_ID)
velH_pest_factor = rcauchy(length(reps),1,0.1), ###get vector of presented stimulus velocities in accordance with staircase (lots of values around PSE, fewer in the periphery)
velH_shown=EffectOfSelfMotion.Accuracy*velH_pest_factor, ###translates from values around 1 to values around PSE
####Get SD for cumulative Gaussian: 0.15*PSE + 0.05*PSE if selfmotion is present, aaaand some inter-subject variability (SD_Factor_ID)
EffectOfSelfMotion.Precision = (abs(EffectOfSelfMotion.Accuracy*0.1)+abs(EffectOfSelfMotion.Accuracy*JND_Diff*(Motion!=0)))*SD_Factor_ID,
####cram everything into a cumulative Gaussian (pnorm()):
AnswerProbability = pnorm(abs(velH_shown),abs(EffectOfSelfMotion.Accuracy),EffectOfSelfMotion.Precision),
####Get difference between target velocity and PEST velocity
Difference = abs(velH_shown) - abs(velH),
##get binary answer from probability for each trial
Answer = as.numeric(rbernoulli(length(AnswerProbability),AnswerProbability))
)
###prepare for glmer() - needs sum of YES/Total per stimulus strength and condition
Psychometric = Psychometric %>%
group_by(ID,Motion,velH,Difference) %>%
mutate(Yes = sum(Answer==1),
Total = length(Motion))
Psychometric =  Psychometric %>%
mutate(Congruent = case_when(
velH*Motion < 0 ~ "incongruent",
velH*Motion > 0 ~ "congruent",
velH*Motion == 0 ~ "1no motion"
)
)
}
Psychometric = Psychometric %>%
filter(abs(Difference) < 5)
ggplot(Psychometric,aes(Difference,Answer,color=Congruent)) +
binomial_smooth() +
facet_grid(velH~ID) +
xlab("Ratio Target/Comparison") +
ylab("Probability Target Bigger") +
coord_cartesian(xlim = c(-5,1))
Analyze_Pychometric_Precision = function(Psychometric){
Psychometric =
select(Psychometric,c(ID,Motion,Yes,Total,Difference,velH)) %>%
distinct() %>%
filter(abs(Difference) < 5)
Psychometric = Psychometric %>% ###for this test, we can collapse right and leftward motion because we expect the same effect in terms of thresholds for both
mutate(
MotionCondition = case_when(
Motion == 0 ~ "No",
Motion != 0 ~ "Yes"
)
)
###precision = slope for Difference (the steeper the slope, the more sensitive) ...
###so if there is an interaction between Motion Condition and Difference, that means that it changes the slope
mod1 = glmer(cbind(Yes, Total - Yes) ~ as.factor(MotionCondition)*Difference + (1 | ID)  + (1 | velH),
family = binomial(link = "probit"),
data = Psychometric)
mod2 = glmer(cbind(Yes, Total - Yes) ~ as.factor(MotionCondition) + Difference + (1 | ID)  + (1 | velH),
family = binomial(link = "probit"),
data = Psychometric)
anova(mod1,mod2)$`Pr(>Chisq)`[2] ##Model 1 beats model 2?
}
Analyze_Pychometric_Accuracy = function(Psychometric){
Psychometric =
select(Psychometric,c(ID,Motion,Yes,Total,Difference, velH,Congruent)) %>%
distinct() %>%
filter(abs(Difference) < 5)
mod1 = glmer(cbind(Yes, Total - Yes) ~ Congruent + Difference + (1 | ID)  + (1 | velH),
family = binomial(link = "probit"),
data = Psychometric)
mod2 = glmer(cbind(Yes, Total - Yes) ~ Difference + (1 | ID) + (1 | velH),
family = binomial(link = "probit"),
data = Psychometric)
anova(mod1,mod2)$`Pr(>Chisq)`[2] ##Model 1 beats model 2
print(anova(mod1,mod2)$`Pr(>Chisq)`[2])
}
######power for very low estimates of effect size: difference of 0.1 m/s in PSEs, and JNDs 1/4 higher when self-motion is simulated
Power_Precision = c()
nIterations = 100
out <- replicate(nIterations, {
Analyze_Pychometric_Precision(SimulatePsychometricFunction(ID=ID, Motion=Motion, velH=velH, reps=reps, PSE_Diff = 0.05, JND_Diff = 0.025))})
hist(out) ###Distribution of p values
Power_Precision = mean(out < 0.05) ###Power is the times the difference between the two models is significant
Power_Precision ###This is the power for the JNDs
#####This is the function from which I choose which comparison values we put into the modelling part:
#super high curtosis because we will get many values around the PSE
DataFrame3 = data.frame(x = rcauchy(1000,1,0.02),
y = rnorm(1000,1,0.1))
ggplot(DataFrame3, aes(x)) + ####just to get an idea of what this function looks like
geom_density() +
coord_cartesian(xlim=c(0.7,1.3))
#####This is the function from which I choose which comparison values we put into the modelling part:
#super high curtosis because we will get many values around the PSE
DataFrame3 = data.frame(x = rcauchy(1000,1,0.02),
y = rnorm(1000,1,0.1))
ggplot(DataFrame3, aes(x)) + ####just to get an idea of what this function looks like
geom_density() +
coord_cartesian(xlim=c(0.7,1.3))
ID = paste0("s",1:16)
Motion = c(-2,0,2) ##motion left, right, and static ... this will be factors that are supposed to represent congruent/incongruent
velH = c(-8,-6.6, 6.6,8) ##target motion to the left (neg values) and to the right (pos values)
reps = seq(1,55,1) ##we have two PESTS, and between 20 and 35 reps for each, averages for 55 for both together
SimulatePsychometricFunction = function(ID,Motion,velH, reps,PSE_Diff, JND_Diff){
Psychometric = expand.grid(ID=ID, Motion=Motion, velH=velH, reps = reps)
Psychometric = Psychometric %>%
group_by(ID) %>%#
mutate(PSE_Factor_ID = rnorm(1,1,0.15),
SD_Factor_ID = rnorm(1,1,0.15))
Psychometric = Psychometric %>%
group_by(ID,Motion,velH) %>%
mutate(
EffectOfSelfMotion.Accuracy = (2*velH/3+Motion*PSE_Diff)*abs(PSE_Factor_ID), ###get PSE: +/- 1/8 of selfmotion, aaaand some inter-subject variability (PSE_Factor_ID)
velH_pest_factor = rcauchy(length(reps),1,0.02), ###get vector of presented stimulus velocities in accordance with staircase (lots of values around PSE, fewer in the periphery)
velH_shown=EffectOfSelfMotion.Accuracy*velH_pest_factor, ###translates from values around 1 to values around PSE
####Get SD for cumulative Gaussian: 0.15*PSE + 0.05*PSE if selfmotion is present, aaaand some inter-subject variability (SD_Factor_ID)
EffectOfSelfMotion.Precision = (abs(EffectOfSelfMotion.Accuracy*0.1)+abs(EffectOfSelfMotion.Accuracy*JND_Diff*(Motion!=0)))*SD_Factor_ID,
####cram everything into a cumulative Gaussian (pnorm()):
AnswerProbability = pnorm(abs(velH_shown),abs(EffectOfSelfMotion.Accuracy),EffectOfSelfMotion.Precision),
####Get difference between target velocity and PEST velocity
Difference = abs(velH_shown) - abs(velH),
##get binary answer from probability for each trial
Answer = as.numeric(rbernoulli(length(AnswerProbability),AnswerProbability))
)
###prepare for glmer() - needs sum of YES/Total per stimulus strength and condition
Psychometric = Psychometric %>%
group_by(ID,Motion,velH,Difference) %>%
mutate(Yes = sum(Answer==1),
Total = length(Motion))
Psychometric =  Psychometric %>%
mutate(Congruent = case_when(
velH*Motion < 0 ~ "incongruent",
velH*Motion > 0 ~ "congruent",
velH*Motion == 0 ~ "1no motion"
)
)
}
Psychometric = Psychometric %>%
filter(abs(Difference) < 5)
ggplot(Psychometric,aes(Difference,Answer,color=Congruent)) +
binomial_smooth() +
facet_grid(velH~ID) +
xlab("Ratio Target/Comparison") +
ylab("Probability Target Bigger") +
coord_cartesian(xlim = c(-5,1))
Analyze_Pychometric_Precision = function(Psychometric){
Psychometric =
select(Psychometric,c(ID,Motion,Yes,Total,Difference,velH)) %>%
distinct() %>%
filter(abs(Difference) < 5)
Psychometric = Psychometric %>% ###for this test, we can collapse right and leftward motion because we expect the same effect in terms of thresholds for both
mutate(
MotionCondition = case_when(
Motion == 0 ~ "No",
Motion != 0 ~ "Yes"
)
)
###precision = slope for Difference (the steeper the slope, the more sensitive) ...
###so if there is an interaction between Motion Condition and Difference, that means that it changes the slope
mod1 = glmer(cbind(Yes, Total - Yes) ~ as.factor(MotionCondition)*Difference + (1 | ID)  + (1 | velH),
family = binomial(link = "probit"),
data = Psychometric)
mod2 = glmer(cbind(Yes, Total - Yes) ~ as.factor(MotionCondition) + Difference + (1 | ID)  + (1 | velH),
family = binomial(link = "probit"),
data = Psychometric)
anova(mod1,mod2)$`Pr(>Chisq)`[2] ##Model 1 beats model 2?
}
Analyze_Pychometric_Accuracy = function(Psychometric){
Psychometric =
select(Psychometric,c(ID,Motion,Yes,Total,Difference, velH,Congruent)) %>%
distinct() %>%
filter(abs(Difference) < 5)
mod1 = glmer(cbind(Yes, Total - Yes) ~ Congruent + Difference + (1 | ID)  + (1 | velH),
family = binomial(link = "probit"),
data = Psychometric)
mod2 = glmer(cbind(Yes, Total - Yes) ~ Difference + (1 | ID) + (1 | velH),
family = binomial(link = "probit"),
data = Psychometric)
anova(mod1,mod2)$`Pr(>Chisq)`[2] ##Model 1 beats model 2
print(anova(mod1,mod2)$`Pr(>Chisq)`[2])
}
######power for very low estimates of effect size: difference of 0.1 m/s in PSEs, and JNDs 1/4 higher when self-motion is simulated
Power_Precision = c()
nIterations = 100
out <- replicate(nIterations, {
Analyze_Pychometric_Precision(SimulatePsychometricFunction(ID=ID, Motion=Motion, velH=velH, reps=reps, PSE_Diff = 0.05, JND_Diff = 0.025))})
hist(out) ###Distribution of p values
Power_Precision = mean(out < 0.05) ###Power is the times the difference between the two models is significant
Power_Precision ###This is the power for the JNDs
######power for very low estimates of effect size: difference of 0.1 m/s in PSEs, and JNDs 1/4 higher when self-motion is simulated
Power_Precision = c()
nIterations = 100
out <- replicate(nIterations, {
Analyze_Pychometric_Precision(SimulatePsychometricFunction(ID=ID, Motion=Motion, velH=velH, reps=reps, PSE_Diff = 1/8, JND_Diff = 0.025))})
hist(out) ###Distribution of p values
Power_Precision = mean(out < 0.05) ###Power is the times the difference between the two models is significant
Power_Precision ###This is the power for the JNDs
###Pull the whole repository and paste the path to the the local GitHub repository here:
require(dplyr)
require(tidyverse)
require(lme4)
library(PearsonDS)
theme_set(theme_cowplot())
set.seed(912)
#####This is the function from which I choose which comparison values we put into the modelling part:
#super high curtosis because we will get many values around the PSE
DataFrame3 = data.frame(x = rcauchy(1000,1,0.02),
y = rnorm(1000,1,0.1))
ggplot(DataFrame3, aes(x)) + ####just to get an idea of what this function looks like
geom_density() +
coord_cartesian(xlim=c(0.7,1.3))
ID = paste0("s",1:16)
Motion = c(-2,0,2) ##motion left, right, and static ... this will be factors that are supposed to represent congruent/incongruent
velH = c(-8,-6.6, 6.6,8) ##target motion to the left (neg values) and to the right (pos values)
reps = seq(1,55,1) ##we have two PESTS, and between 20 and 35 reps for each, averages for 55 for both together
SimulatePsychometricFunction = function(ID,Motion,velH, reps,PSE_Diff, JND_Diff){
Psychometric = expand.grid(ID=ID, Motion=Motion, velH=velH, reps = reps)
Psychometric = Psychometric %>%
group_by(ID) %>%#
mutate(PSE_Factor_ID = rnorm(1,1,0.15),
SD_Factor_ID = rnorm(1,1,0.15))
Psychometric = Psychometric %>%
group_by(ID,Motion,velH) %>%
mutate(
EffectOfSelfMotion.Accuracy = (2*velH/3+Motion*PSE_Diff)*abs(PSE_Factor_ID), ###get PSE: +/- 1/8 of selfmotion, aaaand some inter-subject variability (PSE_Factor_ID)
velH_pest_factor = rcauchy(length(reps),1,0.02), ###get vector of presented stimulus velocities in accordance with staircase (lots of values around PSE, fewer in the periphery)
velH_shown=EffectOfSelfMotion.Accuracy*velH_pest_factor, ###translates from values around 1 to values around PSE
####Get SD for cumulative Gaussian: 0.15*PSE + 0.05*PSE if selfmotion is present, aaaand some inter-subject variability (SD_Factor_ID)
EffectOfSelfMotion.Precision = (abs(EffectOfSelfMotion.Accuracy*0.1)+abs(EffectOfSelfMotion.Accuracy*JND_Diff*(Motion!=0)))*SD_Factor_ID,
####cram everything into a cumulative Gaussian (pnorm()):
AnswerProbability = pnorm(abs(velH_shown),abs(EffectOfSelfMotion.Accuracy),EffectOfSelfMotion.Precision),
####Get difference between target velocity and PEST velocity
Difference = abs(velH_shown) - abs(velH),
##get binary answer from probability for each trial
Answer = as.numeric(rbernoulli(length(AnswerProbability),AnswerProbability))
)
###prepare for glmer() - needs sum of YES/Total per stimulus strength and condition
Psychometric = Psychometric %>%
group_by(ID,Motion,velH,Difference) %>%
mutate(Yes = sum(Answer==1),
Total = length(Motion))
Psychometric =  Psychometric %>%
mutate(Congruent = case_when(
velH*Motion < 0 ~ "incongruent",
velH*Motion > 0 ~ "congruent",
velH*Motion == 0 ~ "1no motion"
)
)
}
Psychometric = Psychometric %>%
filter(abs(Difference) < 5)
ggplot(Psychometric,aes(Difference,Answer,color=Congruent)) +
binomial_smooth() +
facet_grid(velH~ID) +
xlab("Ratio Target/Comparison") +
ylab("Probability Target Bigger") +
coord_cartesian(xlim = c(-5,1))
Analyze_Pychometric_Precision = function(Psychometric){
Psychometric =
select(Psychometric,c(ID,Motion,Yes,Total,Difference,velH)) %>%
distinct() %>%
filter(abs(Difference) < 5)
Psychometric = Psychometric %>% ###for this test, we can collapse right and leftward motion because we expect the same effect in terms of thresholds for both
mutate(
MotionCondition = case_when(
Motion == 0 ~ "No",
Motion != 0 ~ "Yes"
)
)
###precision = slope for Difference (the steeper the slope, the more sensitive) ...
###so if there is an interaction between Motion Condition and Difference, that means that it changes the slope
mod1 = glmer(cbind(Yes, Total - Yes) ~ as.factor(MotionCondition)*Difference + (1 | ID)  + (1 | velH),
family = binomial(link = "probit"),
data = Psychometric)
mod2 = glmer(cbind(Yes, Total - Yes) ~ as.factor(MotionCondition) + Difference + (1 | ID)  + (1 | velH),
family = binomial(link = "probit"),
data = Psychometric)
anova(mod1,mod2)$`Pr(>Chisq)`[2] ##Model 1 beats model 2?
}
Analyze_Pychometric_Accuracy = function(Psychometric){
Psychometric =
select(Psychometric,c(ID,Motion,Yes,Total,Difference, velH,Congruent)) %>%
distinct() %>%
filter(abs(Difference) < 5)
mod1 = glmer(cbind(Yes, Total - Yes) ~ Congruent + Difference + (1 | ID)  + (1 | velH),
family = binomial(link = "probit"),
data = Psychometric)
mod2 = glmer(cbind(Yes, Total - Yes) ~ Difference + (1 | ID) + (1 | velH),
family = binomial(link = "probit"),
data = Psychometric)
anova(mod1,mod2)$`Pr(>Chisq)`[2] ##Model 1 beats model 2
print(anova(mod1,mod2)$`Pr(>Chisq)`[2])
}
######power for very low estimates of effect size: difference of 0.1 m/s in PSEs, and JNDs 1/4 higher when self-motion is simulated
Power_Precision = c()
nIterations = 500
out <- replicate(nIterations, {
Analyze_Pychometric_Precision(SimulatePsychometricFunction(ID=ID, Motion=Motion, velH=velH, reps=reps, PSE_Diff = 1/8, JND_Diff = 0.025))})
hist(out) ###Distribution of p values
Power_Precision = mean(out < 0.05) ###Power is the times the difference between the two models is significant
Power_Precision ###This is the power for the JNDs
Power_Accuracy = c()
nIterations = 500
out2 <- replicate(nIterations, {
Analyze_Pychometric_Accuracy(SimulatePsychometricFunction(ID=ID, Motion=Motion, velH=velH, reps=reps, PSE_Diff = 1/8, JND_Diff = 0.025))})
hist(out2) ###Distribution of p values
Power_Accuracy = mean(out2 < 0.05) ###Power is the times the difference between the two models is significant
Power_Accuracy ###This is the power for the PSEs
Power_Precision
